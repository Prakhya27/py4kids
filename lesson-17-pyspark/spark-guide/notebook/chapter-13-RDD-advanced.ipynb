{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "words.map(lambda word: (word.lower(), 1))\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword = words.keyBy(lambda word: word.lower()[0])\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.mapValues(lambda word: word.upper()).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.flatMapValues(lambda word: word.upper()).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.keys().collect()\n",
    "keyword.values().collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct()\\\n",
    "  .collect()\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "words.map(lambda word: (word.lower()[0], word))\\\n",
    "  .sampleByKey(True, sampleMap, 6).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "chars = words.flatMap(lambda word: word.lower())\n",
    "KVcharacters = chars.map(lambda letter: (letter, 1))\n",
    "def maxFunc(left, right):\n",
    "  return max(left, right)\n",
    "def addFunc(left, right):\n",
    "  return left + right\n",
    "nums = sc.parallelize(range(1,31), 5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.countByKey()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1])))\\\n",
    "  .collect()\n",
    "# note this is Python 2, reduce must be imported from functools in Python 3\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "nums.aggregate(0, maxFunc, addFunc)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "depth = 3\n",
    "nums.treeAggregate(0, maxFunc, addFunc, depth)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.aggregateByKey(0, addFunc, maxFunc).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def valToCombiner(value):\n",
    "  return [value]\n",
    "def mergeValuesFunc(vals, valToAppend):\n",
    "  vals.append(valToAppend)\n",
    "  return vals\n",
    "def mergeCombinerFunc(vals1, vals2):\n",
    "  return vals1 + vals2\n",
    "outputPartitions = 6\n",
    "KVcharacters\\\n",
    "  .combineByKey(\n",
    "    valToCombiner,\n",
    "    mergeValuesFunc,\n",
    "    mergeCombinerFunc,\n",
    "    outputPartitions)\\\n",
    "  .collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.foldByKey(0, addFunc).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "charRDD = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD2 = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD.cogroup(charRDD2).take(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyedChars = distinctChars.map(lambda c: (c, random.random()))\n",
    "outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "numRange = sc.parallelize(range(10), 2)\n",
    "words.zip(numRange).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "words.coalesce(1).getNumPartitions() # 1\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/data/retail-data/all/\")\n",
    "rdd = df.coalesce(10).rdd\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def partitionFunc(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    return 0\n",
    "  else:\n",
    "    return random.randint(1,2)\n",
    "\n",
    "keyedRDD = rdd.keyBy(lambda row: row[6])\n",
    "keyedRDD\\\n",
    "  .partitionBy(3, partitionFunc)\\\n",
    "  .map(lambda x: x[0])\\\n",
    "  .glom()\\\n",
    "  .map(lambda x: len(set(x)))\\\n",
    "  .take(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
