{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/data/flight-data/csv/2010-summary.csv\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    "  .save(\"/tmp/my-tsv-file.tsv\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/data/flight-data/json/2010-summary.json\").show(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.format(\"parquet\")\\\n",
    "  .load(\"/data/flight-data/parquet/2010-summary.parquet\").show(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    "  .save(\"/tmp/my-parquet-file.parquet\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.format(\"orc\").load(\"/data/flight-data/orc/2010-summary.orc\").show(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-json-file.orc\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"/data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\"\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "  .option(\"dbtable\", tablename).option(\"driver\",  driver).load()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "pgDF = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "  .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "  .option(\"dbtable\", \"schema.tablename\")\\\n",
    "  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')\").explain()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)\n",
    "  AS flight_info\"\"\"\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\",  driver)\\\n",
    "  .load()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\",  driver)\\\n",
    "  .option(\"numPartitions\", 10).load()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
    "spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\\\n",
    "  .rdd.getNumPartitions() # 2\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "colName = \"count\"\n",
    "lowerBound = 0L\n",
    "upperBound = 348113L # this is the max count in our database\n",
    "numPartitions = 10\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.jdbc(url, tablename, column=colName, properties=props,\n",
    "                lowerBound=lowerBound, upperBound=upperBound,\n",
    "                numPartitions=numPartitions).count() # 255\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.jdbc(newPath, tablename, properties=props).count() # 255\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark.read.jdbc(newPath, tablename, properties=props).count() # 765\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\\\n",
    "  .write.partitionBy(\"count\").text(\"/tmp/five-csv-files2py.csv\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .save(\"/tmp/partitioned-files.parquet\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
